{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pima = np.genfromtxt('pima-indians-diabetes.data', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def sghmc(Y, X, stogradU, M, eps, m, theta, C, V):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Randomly sample momentum\n",
    "    r = np.random.multivariate_normal(np.zeros(M.shape[0]),M)[:,np.newaxis]\n",
    "    \n",
    "    # Precompute\n",
    "    B = 0.5 * V * eps\n",
    "    D = 2*(C-B)*eps\n",
    "    Minv = np.linalg.inv(M)\n",
    "    \n",
    "    # Hamiltonian dynamics\n",
    "    for i in range(m):\n",
    "        theta = theta + (eps*np.linalg.inv(M) @ r).ravel()\n",
    "        r = r - eps*stogradU(theta, Y, X, nbatch) - eps*C @ Minv @ r \\\n",
    "            + np.random.multivariate_normal(np.zeros(M.shape[0]),D)[:,np.newaxis]\n",
    "\n",
    "    return(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def stogradU(theta, Y, X, nbatch):\n",
    "    '''A function that returns the stochastic gradient. Adapted from Eq. 5.\n",
    "    Inputs are:\n",
    "        theta, the parameters\n",
    "        Y, the response\n",
    "        X, the covariates\n",
    "        nbatch, the number of samples to take from the full data\n",
    "    '''\n",
    "    alpha=5\n",
    "    n = X.shape[0]\n",
    "    batch_id = np.random.choice(np.arange(n),nbatch,replace=False)\n",
    "    grad = -n/nbatch * X[batch_id,:].T @ (Y[batch_id][:,np.newaxis] - \\\n",
    "        1/(1+np.exp(-X[batch_id,:] @ theta[:,np.newaxis]))) - theta[:,np.newaxis]/alpha\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def stogradU(theta, Y, X, nbatch):\n",
    "    '''A function that returns the stochastic gradient. Adapted from Eq. 5.\n",
    "    Inputs are:\n",
    "        theta, the parameters\n",
    "        Y, the response\n",
    "        X, the covariates\n",
    "        nbatch, the number of samples to take from the full data\n",
    "    '''\n",
    "    alpha=5\n",
    "    n = X.shape[0]\n",
    "    batch_id = np.random.choice(np.arange(n),nbatch,replace=False)\n",
    "    \n",
    "    Y_pred = logistic(X[batch_id,:] @ theta[:,np.newaxis])\n",
    "    epsilon = (Y[batch_id][:,np.newaxis] - Y_pred)\n",
    "    grad = -n/nbatch * X[batch_id,:].T @ epsilon - theta[:,np.newaxis]/alpha\n",
    "    \n",
    "    return grad/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "X = np.concatenate((np.ones((pima.shape[0],1)),pima[:,0:8]), axis=1)\n",
    "Y = pima[:,8]\n",
    "\n",
    "Xs = (X - np.mean(X, axis=0))/np.concatenate((np.ones(1),np.std(X[:,1:], axis=0)))\n",
    "n, p = X.shape\n",
    "\n",
    "nsample = 1\n",
    "nbatch = 768\n",
    "M = np.identity(p)\n",
    "C = 0 * np.identity(p)\n",
    "eps = 0.1\n",
    "m = 10\n",
    "V = 0 * np.identity(p)\n",
    "theta = np.zeros(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HMC version\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def gradU(theta, Y, X, nbatch):\n",
    "    '''A function that returns the stochastic gradient. Adapted from Eq. 5.\n",
    "    Inputs are:\n",
    "        theta, the parameters\n",
    "        Y, the response\n",
    "        X, the covariates\n",
    "        nbatch, the number of samples to take from the full data\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    Y_pred = logistic(X @ theta)\n",
    "    epsilon = (Y[:,np.newaxis] - Y_pred[:,np.newaxis])\n",
    "    grad = X.T @ epsilon\n",
    "\n",
    "    return -grad/n\n",
    "    #temp = -grad/n\n",
    "    #return temp / np.linalg.norm(temp)\n",
    "\n",
    "\n",
    "def hmc(Y, X, gradU, M, eps, m, theta, C, V):\n",
    "    # This is just HMC for testing\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Randomly sample momentum\n",
    "    r = np.random.multivariate_normal(np.zeros(M.shape[0]),M)[:,np.newaxis]\n",
    "    \n",
    "    # Precompute\n",
    "    Minv = np.linalg.inv(M)\n",
    "    \n",
    "    # Hamiltonian dynamics\n",
    "    r = r - (eps/2)*gradU(theta, Y, X, nbatch)\n",
    "    for i in range(m):\n",
    "        theta = theta + (eps*Minv@r).ravel()\n",
    "        r = r - eps*gradU(theta, Y, X, nbatch)\n",
    "    return theta\n",
    "\n",
    "def my_gd(Y, X, gradU, M, eps, m, theta, C, V):\n",
    "    # gradient descent\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    for i in range(m):\n",
    "        theta = theta - eps*gradU(theta, Y, X, nbatch).ravel()\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.31498612e+00,   1.22560027e-01,   3.49183220e-02,\n",
       "        -1.34118967e-02,   6.28219471e-04,  -1.17179659e-03,\n",
       "         8.86606033e-02,   9.30419443e-01,   1.46781178e-02])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unscaled\n",
    "mod_logis = LogisticRegression(fit_intercept=False, C=1e50)\n",
    "mod_logis.fit(X,Y)\n",
    "beta_true_unscale = mod_logis.coef_.ravel()\n",
    "beta_true_unscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.39024907,  1.08791914, -0.24544979,  0.02250608,\n",
       "       -0.1621995 ,  0.59035938,  0.32483104,  0.12120845])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled\n",
    "mod_logis = LogisticRegression(fit_intercept=False, C=1e50)\n",
    "mod_logis.fit(Xs,Y)\n",
    "beta_true_scale = mod_logis.coef_.ravel()\n",
    "beta_true_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our code - HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08139515,  0.00343468, -0.04585262,  0.4752753 , -0.30726844,\n",
       "        0.13273856,  0.18517392, -0.24758163, -0.11967343])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HMC - Unscaled\n",
    "nsample = 1\n",
    "m = 2\n",
    "\n",
    "np.random.seed(2)\n",
    "samples = np.zeros((nsample, p))\n",
    "theta = np.zeros(p)\n",
    "for i in range(nsample):\n",
    "    theta = hmc(Y, X, gradU, M, eps, m, theta, C, V)\n",
    "    samples[i] = theta\n",
    "    \n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.79430421, -0.74558443,  0.18226611,  1.6872832 , -4.72043896,\n",
       "       -0.6984265 ,  0.91224167, -2.25007933,  3.21615521])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HMC - Scaled\n",
    "nsample = 10\n",
    "m = 10\n",
    "\n",
    "np.random.seed(2)\n",
    "samples = np.zeros((nsample, p))\n",
    "theta = np.zeros(p)\n",
    "for i in range(nsample):\n",
    "    theta = hmc(Y, Xs, gradU, M, eps, m, theta, C, V)\n",
    "    samples[i] = theta\n",
    "    \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our code - Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.19413632e-04,   2.36331548e-04,   7.48727924e-06,\n",
       "         2.31994484e-06,   9.33934233e-06,  -3.92206222e-07,\n",
       "        -2.15577090e-05,   1.00758322e-04,  -4.00087824e-05])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent - Unscaled\n",
    "np.random.seed(2)\n",
    "#res = my_gd(Y, X, gradU, M, .0001, 10000, np.zeros(p), C, V) # Starting at zero\n",
    "#res = my_gd(Y, X, gradU, M, .0001, 10000, beta_true_unscale.copy(), C, V) # Starting at true values\n",
    "\n",
    "res = my_gd(Y, X, gradU, M, .0001, 10000, beta_true_unscale.copy(), C, V) # Starting at true values\n",
    "\n",
    "res - beta_true_unscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   4.17596046e-06,   5.92833240e-06,\n",
       "         3.17764100e-06,   8.84196257e-06,  -2.98662348e-06,\n",
       "        -1.76233441e-05,   8.02028628e-06,  -7.35280641e-06])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent - Scaled\n",
    "np.random.seed(2)\n",
    "res = my_gd(Y, Xs, gradU, M, eps, 20000, np.zeros(p), C, V)\n",
    "\n",
    "res - beta_true_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliburn's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cliburn's gradient descent code\n",
    "\n",
    "def gd(X, y, beta, alpha, niter):\n",
    "    \"\"\"Gradient descent algorihtm.\"\"\"\n",
    "    n, p = X.shape\n",
    "    Xt = X.T\n",
    "    for i in range(niter):\n",
    "        y_pred = logistic(X @ beta)\n",
    "        epsilon = y - y_pred\n",
    "        grad = Xt @ epsilon / n\n",
    "        beta += alpha * grad\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.19413632e-04,   2.36331548e-04,   7.48727924e-06,\n",
       "         2.31994484e-06,   9.33934233e-06,  -3.92206222e-07,\n",
       "        -2.15577090e-05,   1.00758322e-04,  -4.00087824e-05])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unscaled\n",
    "#res = gd(X, Y.ravel(), np.zeros(p), alpha=.1, niter=2) # Starting at zero\n",
    "res = gd(X, Y.ravel(), beta_true_unscale.copy(), alpha=.0001, niter=10000) # Starting at true coefficients\n",
    "\n",
    "res - beta_true_unscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   4.17596046e-06,   5.92833240e-06,\n",
       "         3.17764100e-06,   8.84196257e-06,  -2.98662348e-06,\n",
       "        -1.76233441e-05,   8.02028628e-06,  -7.35280641e-06])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled\n",
    "res = gd(Xs, Y.ravel(), np.zeros(p), alpha=.1, niter=20000)\n",
    "\n",
    "res - beta_true_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
