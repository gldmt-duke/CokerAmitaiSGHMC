Any MCMC algorithm is inherently sequential, and so can't be parallelized (though multiple chains can be run at the same time). Each parameter's full conditional distribution depends on other parameters, so loops are a natural implementation. Unfortunately, loops are quite slow in Python. On the other hand, a compiled language has no such issues. In this section, we discuss an alternative, optimized version of our code that is written in C++ and is callable from Python via the Pybind11 package.

We feel this is a nice Both libraries are written with the same input syntax and function names, so they can be easily exchanged. A small difference is that the potential energy function and gradient function cannot be supplied by the user in the C++ implementation.\\

Note that one drawback to a C++ implementation is the limited availability of easy-to-use random sampling functions. To complete our algorithm, we wrote a random multivariate normal function based on a Cholesky decomposition (necessary for the momentum updates and noise terms) and a random sampling function (necessary for the stochastic gradient descent). Unfortunately, the random sampling function is quite slow, significantly dampening the impact from the low-level speedup.

To compare the performance of our two implementations, we make use of the {\tt %timeit} magic function. Unfortunately, while the {\tt %prun} profiler provides useful information on the Python implementation, it does not work on the Pybind11 implementation.
	
Starting with the stochastic gradient function, we see that...

Next, we see that the {\tt run_sghmc} function

As discussed, {\tt %prun} will not work for the Pybind11 version, so we can only examine the results for the unoptimized Python version.