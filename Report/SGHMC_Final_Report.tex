\documentclass[12pt]{article}
\usepackage[letterpaper, scale=0.85]{geometry} % Reduce document margins
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{algorithm2e}
\usepackage{float}
\usepackage{hyperref}


%opening
\setlength{\parindent}{0pt}

% Custome latex commands
\newcommand{\prob}[1]{\mathrm{Pr}\left(#1\right)}
\newcommand{\braces}[1]{\left\lbrace#1 \right\rbrace}

\begin{document}
\vspace{-1in}
\author{\bf Sta663 Final Project}
\title{\bf Stochastic Gradient Descent Hamiltonian Monte Carlo Applied to Bayesian Logistic Regression}
\date{Gilad Amitai and Beau Coker}
%\date{\today}
\maketitle 
%\thispagestyle{empty}


\begin{abstract}
	Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo algorithm for drawing samples from a probability distribution where proposed values are computed using Hamiltonian dynamics to find values of high acceptance probabilities. They allow us to explore sample states more efficiently than random walk proposals, but are limited by the expensive computation of the gradient of the potential energy function. Chen, Fox, and Guestrin propose the method Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), a HMC algorithm that uses a subset of the data to compute the gradient. The authors find that the stochastic gradient is noisy and correct this with a friction term.

	In this project, we adapt the SGHMC to be used for Bayesian Logistic regression, implement this method in Python, optimize the code for computational efficiency, validate our approach using simulated data, and apply the algorithm to real world classification problems.
	
	All code for this project is freely available on  at\url{https://github.com/gldmt-duke/CokerAmitaiSGHMC}{Github}.
	
	Keywords: Hamiltonian Monte Carlo, Stochastic Gradient Hamiltonian Monte Carlo, Pima Indians Diabetes Dataset, Hockey Puck, Logistic Regression, Markov chain Monte Carlo
\end{abstract}

\section{Background}
\input{background.tex}

\section{Description of Algorithm}
\input{description.tex}

\section{Optimization}
\input{optimization.tex}

\section{Application to Simulated Data}
\input{simulated.tex}

\section{Application to Real Data}
\input{real.tex}

%\section{Comparative Analysis}
%\input{comparative.tex}

\section{Discussion and Conclusion}
\input{discussion.tex}

\section{Bibliography}

\begin{thebibliography}{9}
	\bibitem{paper} 
	Chen, Fox, Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. \textit{Proceedings of the 31st International Conference on Machine Learning}, Beijing, China, 2014. JMLR: W\&CP volume 32.
	
	\bibitem{neal} 
	Neal, R.M. MCMC using Hamiltonian dynamics. \textit{Handbook of Markov Chain Monte Carlo}, 54:113-162, 2010.
	
	\bibitem{split} 
	Shahbaba, Lan, Johnson, Neal. Split Hamiltonian Monte Carlo. 2012.
	
	\bibitem{metropolis}
	Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953), ``Equation of State Calculations by Fast Computing Machines,'' \textit{The Journal of Chemical Physics}, 21, 1087â€“1092.
\end{thebibliography}

\end{document}